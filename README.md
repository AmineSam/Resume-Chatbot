# ğŸ’¼ Resume Chatbot

A RAG-based (Retrieval-Augmented Generation) chatbot that answers questions about Amine Samoudi's professional experience using natural language.

## âœ¨ Features

- **Dual-Mode Support**: Run locally with Ollama or in the cloud with Groq
- **Intelligent Retrieval**: Uses FAISS vector store with HuggingFaceEmbeddings
- **Synthesis-Focused**: Generates conversational answers, not copy-paste chunks
- **Chat Memory**: Maintains conversation context for follow-up questions
- **Clean UI**: Streamlit-based interface with chat history

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚   LLM    â”‚  â—„â”€â”€ MODE=LOCAL â†’ Ollama (llama3.2)
    â”‚ Provider â”‚  â—„â”€â”€ MODE=CLOUD â†’ Groq (llama-3.1-70b)
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangChain RAG      â”‚
â”‚  - Retriever (FAISS)â”‚
â”‚  - Memory           â”‚
â”‚  - Custom Prompt    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HuggingFace         â”‚
â”‚ Embeddings          â”‚
â”‚ (all-MiniLM-L6-v2)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- For LOCAL mode: [Ollama](https://ollama.ai) installed
- For CLOUD mode: [Groq API key](https://console.groq.com)

### Installation

1. **Clone and navigate to the repository**
```bash
cd Resume-Chatbot
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure environment**
```bash
# Copy the example file
cp .env.example .env

# Edit .env with your preferred settings
```

### Running the Chatbot

#### Option 1: LOCAL Mode (Ollama)

1. **Install Ollama** from [ollama.ai](https://ollama.ai)

2. **Pull the llama3.2 model**
```bash
ollama pull llama3.2
```

3. **Start Ollama server** (if not running)
```bash
ollama serve
```

4. **Set MODE in .env**
```env
MODE=LOCAL
```

5. **Run the app**
```bash
streamlit run app.py
```

#### Option 2: CLOUD Mode (Groq)

1. **Get a free Groq API key** from [console.groq.com](https://console.groq.com)

2. **Set environment variables in .env**
```env
MODE=CLOUD
GROQ_API_KEY=your_actual_api_key_here
```

3. **Run the app**
```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

## ğŸ§ª Testing

Run the test script to validate your setup:

```bash
python test_bot.py
```

This will:
- âœ… Verify resume data loading
- âœ… Test embeddings generation
- âœ… Validate LLM connectivity
- âœ… Check the RAG chain logic

## ğŸ“ Project Structure

```
Resume-Chatbot/
â”œâ”€â”€ app.py              # Main Streamlit application
â”œâ”€â”€ info.txt            # Resume data (source document)
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env.example        # Environment variable template
â”œâ”€â”€ .env               # Your local config (git-ignored)
â”œâ”€â”€ test_bot.py        # Test suite
â””â”€â”€ README.md          # This file
```

## ğŸ’¡ Example Questions

Try asking:
- "What is Amine's educational background?"
- "Tell me about the Immo-Eliza project"
- "What technologies does he work with?"
- "What was his role at Unilin?"
- "What certifications does he have?"

## ğŸ”§ Troubleshooting

### LOCAL Mode Issues

**Error: "Ollama connection failed"**
- Make sure Ollama is running: `ollama serve`
- Verify the model is pulled: `ollama pull llama3.2`
- Check Ollama is accessible at `http://localhost:11434`

### CLOUD Mode Issues

**Error: "GROQ_API_KEY not found"**
- Ensure `.env` file exists and contains your API key
- Verify the key is valid at [console.groq.com](https://console.groq.com)

### General Issues

**Error: "Resume file 'info.txt' not found"**
- Ensure `info.txt` is in the same directory as `app.py`

**Slow first response**
- First run downloads the embedding model (~90MB)
- Subsequent runs use cached embeddings

## ğŸŒ Deployment

### Streamlit Cloud (Recommended)

For detailed deployment instructions with **security best practices**, see:

ğŸ“– **[DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)** - Complete guide for deploying to Streamlit Cloud

**Quick Summary**:
1. Add `info.txt` to `.gitignore` (keep your resume private)
2. Push code to public GitHub repository
3. Deploy on [share.streamlit.io](https://share.streamlit.io/)
4. Add resume content to **Streamlit Secrets** (encrypted storage)
5. App loads from secrets in cloud, from file locally

### Alternative: Render

For Render deployment:

1. **Create a `render.yaml`**
```yaml
services:
  - type: web
    name: resume-chatbot
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: streamlit run app.py --server.port $PORT
    envVars:
      - key: MODE
        value: CLOUD
      - key: GROQ_API_KEY
        sync: false  # Set in Render dashboard
```

2. **Set environment variables in Render dashboard**
   - `MODE=CLOUD`
   - `GROQ_API_KEY=your_key`

3. **Deploy** via Render's GitHub integration

## ğŸ”’ Security Note

**Important**: `info.txt` contains personal information and should **NOT** be committed to GitHub.

- âœ… `info.txt` is in `.gitignore`
- âœ… Use `info.txt.example` as a template
- âœ… For deployment, use Streamlit Secrets or environment variables
- âœ… Never commit API keys or personal data

See [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) for complete security setup.

## ğŸ“ License

MIT License - feel free to use this for your own resume chatbot!

## ğŸ¤ Contributing

This is a personal project, but suggestions are welcome via issues.

---

**Built with**: LangChain â€¢ Streamlit â€¢ HuggingFace â€¢ FAISS â€¢ Ollama/Groq
